<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 2 - SVD LoRA Adapter</title>

    <!-- MathJax for rendering LaTeX math -->
    <script type="text/javascript" async 
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <style>
        body {
            font-family: system-ui, -apple-system, sans-serif;
            line-height: 1.6;
            color: #333;
            padding: 2rem;
            max-width: 800px;
            margin: 0 auto;
            background-color: #f9f9f9;
        }

        h1 {
            text-align: center;
            margin-bottom: 1rem;
            color: #333;
        }

        p {
            margin: 1rem 0;
        }

        figure {
            text-align: center;
            margin-top: 1rem;
        }

        figcaption {
            font-size: 14px;
            color: #555;
            margin-top: 0.5rem;
        }

        a {
            text-decoration: none;
            color: #fff;
            font-weight: 500;
            display: inline-block;
            margin-top: 1rem;
            padding: 0.5rem 1rem;
            background-color: #333;
            border-radius: 5px;
            text-align: center;
        }

        a:hover {
            background-color: #555;
        }
    </style>
</head>
<body>
    <h1><strong>Project 2 - SVD LoRA Adapter</strong></h1>

    <p>This project was conducted as part of my role as a <strong>Research Assistant</strong> under <strong>Dr. Jie Chen</strong> at the <strong>MIT AI Watson Lab</strong>. My focus was on the efficient implementation of <strong>Low-Rank Approximation (LoRA)</strong> techniques using adapter methods.</p>

    <p>My responsibilities included developing and optimizing algorithms to <strong>reduce computational complexity</strong> while maintaining accuracy, enabling <strong>faster data processing</strong> and improving <strong>machine learning model efficiency</strong>.</p>

    <h2><strong>1. Original LoRA Method</strong></h2>
    <p>The original <strong>LoRA method</strong> reduces the total number of trainable parameters by decomposing the weight matrix into low-rank matrices. This is represented as:</p>

    <p>
        \[
        W = A \times B
        \]
    </p>

    <p>where <strong>A</strong> and <strong>B</strong> are low-rank matrices. This transformation allows models to adapt to new tasks with minimal modifications to the original architecture, significantly reducing the number of parameters that need fine-tuning. As a result, <strong>LoRA</strong> enables <strong>efficient transfer learning</strong> and <strong>task adaptation</strong>.</p>

    <figure>
        <img src="images/Lora.png" alt="LoRA Architecture" style="width: 100%; height: auto;">
        <figcaption>
            <strong>Figure 1:</strong> LoRA Architecture. Adapted from <em>Edward J., et al.</em>
        </figcaption>
    </figure>

    <p>By applying <strong>LoRA adapters</strong> to models, we can <strong>reduce the number of trainable parameters</strong> to as little as <strong>1/1000 of the original model</strong> (e.g., from <strong>10 billion to 3 million</strong> trainable parameters) while simultaneously <strong>improving model performance</strong>.</p>

    <h2><strong>2. Our Method: SVD-LoRA Decomposition</strong></h2>
    <p>In our approach, we propose a novel decomposition of the weight matrix <strong>W</strong> into three components:</p>

    <p>
        \[
        W = P \times S \times \Pi
        \]
    </p>

    <p>where:</p>
    <ul>
        <li><strong>P</strong> ∈ ℝ<sup>d × r</sup> (low-rank projection matrix)</li>
        <li><strong>S</strong> ∈ ℝ<sup>r × r</sup> (small core matrix)</li>
        <li><strong>Π</strong> ∈ ℝ<sup>r × d</sup> (low-rank reconstruction matrix)</li>
    </ul>

    <p>Here is the architecture of our method:</p>

    <figure>
        <img src="images/SVD_lora.png" alt="SVD_LoRA Architecture" style="width: 100%; height: auto;">
        <figcaption>
            <strong>Figure 2:</strong> SVD-LoRA Architecture.
        </figcaption>
    </figure>

    <h2><strong>3. Underlying Principles</strong></h2>
    <p>Our idea is inspired by <strong>Kronecker matrix product</strong> and <strong>multigrid methods</strong>. The process involves:</p>
    <ol>
        <li>Performing <strong>balanced clustering</strong> on the original weight matrix <strong>W</strong>.</li>
        <li>Assigning elements of <strong>W</strong> to corresponding clusters.</li>
        <li>Initializing:
            \[
            P = \mathcal{N}(0,1), \quad \Pi = \mathcal{N}(0, 1/k)
            \]
            where <strong>k</strong> is the number of elements in each cluster, and
            \[
            S = 0.
            \]
        </li>
    </ol>

    <h2><strong>4. Training Strategy</strong></h2>
    <p>Unlike traditional <strong>LoRA</strong>, we only allow <strong>S</strong> to be trainable while freezing <strong>P</strong> and <strong>Π</strong>. This significantly reduces the number of trainable parameters while maintaining model performance.</p>

    <p>For example, our method further reduces the trainable parameters from <strong>3 million to 30 thousand</strong>, making it even more efficient for <strong>transfer learning</strong> and <strong>model adaptation tasks</strong>.</p>

    <h2><strong>Reference</strong></h2>
    <p>Hu, Edward J., et al. <em>"LoRA: Low-Rank Adaptation of Large Language Models."</em> 2021, arXiv, <a href="https://arxiv.org/abs/2106.09685" target="_blank">https://arxiv.org/abs/2106.09685</a>.</p>

    <div style="text-align: center;">
        <a href="projects.html">Back to Projects</a>
    </div>

</body>
</html>
